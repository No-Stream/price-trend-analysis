{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - BaT Scraping\n",
    "\n",
    "Scrape completed Porsche 911 auctions from Bring a Trailer.\n",
    "\n",
    "**Outputs:**\n",
    "- `data/raw/bat_listings.parquet` - Raw scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\nimport pandas as pd\nfrom pathlib import Path\n\nfrom price_analysis.constants import SCRAPING_DEFAULTS\nfrom price_analysis.scraping import fetch_auctions, validate_scraped_data, DataQualityError\nfrom price_analysis.scraping.bat import listings_to_dataframe\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH = RAW_DIR / \"bat_listings.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Scraping\n",
    "\n",
    "Adjust search queries and pagination as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search queries - multiple generation-specific searches to get more results\n# BaT seems to limit how many results a single search returns\nQUERIES = [\n    # \"Porsche 996\",  # 1999-2004 (996.1 and 996.2)\n    # \"Porsche 997\",  # 2005-2012 (997.1 and 997.2)\n    \"Porsche 991\",  # 2012-2019 (991.1 and 991.2)\n    \"Porsche 992\",  # 2020+ (992.1 and 992.2)\n    \"Porsche 911\",  # Catch air-cooled + anything else missed\n]\n\n# Scraping parameters (from centralized constants)\nMAX_CLICKS = SCRAPING_DEFAULTS[\"max_clicks\"]\nDELAY = SCRAPING_DEFAULTS[\"delay\"]\nHEADLESS = SCRAPING_DEFAULTS[\"headless\"]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: Inspect BaT Page Structure (Optional)\n",
    "\n",
    "Skip this section - fixture files already saved to `tests/fixtures/`.\n",
    "Only re-run if BaT changes their DOM structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Save sample pages for selector inspection\n",
    "import time\n",
    "from price_analysis.scraping import create_driver, save_debug_page\n",
    "\n",
    "# Create driver (set HEADLESS=False above to watch)\n",
    "driver = create_driver(headless=HEADLESS)\n",
    "\n",
    "try:\n",
    "    # 1. Fetch search results page\n",
    "    search_url = \"https://bringatrailer.com/auctions/results/?s=Porsche+911\"\n",
    "    logger.info(f\"Fetching search page: {search_url}\")\n",
    "    driver.get(search_url)\n",
    "    time.sleep(3)  # Wait for JS to load\n",
    "    save_debug_page(driver, \"bat_search_page\", output_dir=\"../tests/fixtures\")\n",
    "\n",
    "    # 2. Fetch a specific listing page (992.1)\n",
    "    listing_url = \"https://bringatrailer.com/listing/2020-porsche-911-carrera-4s-coupe-26/\"\n",
    "    logger.info(f\"Fetching listing: {listing_url}\")\n",
    "    driver.get(listing_url)\n",
    "    time.sleep(3)\n",
    "    save_debug_page(driver, \"bat_listing_992\", output_dir=\"../tests/fixtures\")\n",
    "\n",
    "    # 3. Fetch another listing (997.2) for variety\n",
    "    listing_url_997 = \"https://bringatrailer.com/listing/2009-porsche-911-carrera-4s-coupe-28/\"\n",
    "    logger.info(f\"Fetching listing: {listing_url_997}\")\n",
    "    driver.get(listing_url_997)\n",
    "    time.sleep(3)\n",
    "    save_debug_page(driver, \"bat_listing_997\", output_dir=\"../tests/fixtures\")\n",
    "\n",
    "    logger.info(\"Debug pages saved to tests/fixtures/\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Scraper\n",
    "\n",
    "This will take a while depending on MAX_PAGES. Each page + listing takes ~3-5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing URLs for incremental scraping (skip already-fetched listings)\n",
    "existing_urls: set[str] = set()\n",
    "if OUTPUT_PATH.exists():\n",
    "    existing_df = pd.read_parquet(OUTPUT_PATH)\n",
    "    existing_urls = set(existing_df[\"listing_url\"])\n",
    "    logger.info(f\"Loaded {len(existing_urls)} existing URLs - will skip these\")\n",
    "\n",
    "all_listings = []\n",
    "\n",
    "for query in QUERIES:\n",
    "    logger.info(f\"Scraping: {query}\")\n",
    "    listings = fetch_auctions(\n",
    "        query=query,\n",
    "        max_clicks=MAX_CLICKS,\n",
    "        delay=DELAY,\n",
    "        headless=HEADLESS,\n",
    "        existing_urls=existing_urls,\n",
    "    )\n",
    "    all_listings.extend(listings)\n",
    "    logger.info(f\"Found {len(listings)} NEW listings for '{query}'\")\n",
    "\n",
    "logger.info(f\"Total NEW listings scraped: {len(all_listings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = listings_to_dataframe(all_listings)\n",
    "\n",
    "# Run data quality checks and filter low-price listings (<$10k)\n",
    "# Returns filtered DataFrame, raises DataQualityError if checks fail\n",
    "df = validate_scraped_data(df)\n",
    "\n",
    "display(df.head(10))\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to existing if present, otherwise create new\n",
    "if OUTPUT_PATH.exists():\n",
    "    existing = pd.read_parquet(OUTPUT_PATH)\n",
    "    df = pd.concat([existing, df], ignore_index=True)\n",
    "    df = df.drop_duplicates(subset=[\"listing_url\"], keep=\"last\")\n",
    "    logger.info(f\"Merged with existing data: {len(df)} total listings\")\n",
    "\n",
    "df.to_parquet(OUTPUT_PATH, index=False)\n",
    "logger.info(f\"Saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts by generation:\")\n",
    "display(df[\"generation\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts by trim:\")\n",
    "display(df[\"trim\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts by transmission:\")\n",
    "display(df[\"transmission\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check parsing quality - how many have all required fields?\n",
    "required = [\"sale_price\", \"model_year\", \"generation\", \"trim\", \"transmission\", \"mileage\"]\n",
    "complete = df[required].notna().all(axis=1).sum()\n",
    "print(\n",
    "    f\"\\nListings with all required fields: {complete} / {len(df)} ({complete / len(df) * 100:.1f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some listings to verify parsing\n",
    "print(\"Sample listings for manual verification:\")\n",
    "sample = df.sample(min(10, len(df)), random_state=42)\n",
    "for _, row in sample.iterrows():\n",
    "    print(f\"\\n{row['title_raw']}\")\n",
    "    print(\n",
    "        f\"  Parsed: {row['model_year']} {row['generation']} {row['trim']} ({row['transmission']})\"\n",
    "    )\n",
    "    print(f\"  Price: ${row['sale_price']:,}\" if pd.notna(row[\"sale_price\"]) else \"  Price: N/A\")\n",
    "    print(f\"  Mileage: {row['mileage']:,}\" if pd.notna(row[\"mileage\"]) else \"  Mileage: N/A\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
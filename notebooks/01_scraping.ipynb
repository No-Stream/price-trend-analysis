{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - BaT Scraping\n",
    "\n",
    "Scrape completed Porsche 911 auctions from Bring a Trailer.\n",
    "\n",
    "**Outputs:**\n",
    "- `data/raw/bat_listings.parquet` - Raw scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from price_analysis.scraping import fetch_auctions\n",
    "from price_analysis.scraping.bat import listings_to_dataframe\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH = RAW_DIR / \"bat_listings.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Scraping\n",
    "\n",
    "Adjust search queries and pagination as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search queries - can be refined for specific generations\n",
    "QUERIES = [\n",
    "    \"Porsche 911\",  # Broad search to get all 911s\n",
    "]\n",
    "\n",
    "# Scraping parameters\n",
    "MAX_PAGES = 50  # Adjust based on how much data you want\n",
    "DELAY = 2.5     # Seconds between requests (be polite!)\n",
    "HEADLESS = True # Set False to see browser for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Scraper\n",
    "\n",
    "This will take a while depending on MAX_PAGES. Each page + listing takes ~3-5 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_listings = []\n",
    "\n",
    "for query in QUERIES:\n",
    "    logger.info(f\"Scraping: {query}\")\n",
    "    listings = fetch_auctions(\n",
    "        query=query,\n",
    "        max_pages=MAX_PAGES,\n",
    "        delay=DELAY,\n",
    "        headless=HEADLESS,\n",
    "    )\n",
    "    all_listings.extend(listings)\n",
    "    logger.info(f\"Found {len(listings)} listings for '{query}'\")\n",
    "\n",
    "logger.info(f\"Total listings scraped: {len(all_listings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = listings_to_dataframe(all_listings)\n",
    "display(df.head(10))\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to existing if present, otherwise create new\n",
    "if OUTPUT_PATH.exists():\n",
    "    existing = pd.read_parquet(OUTPUT_PATH)\n",
    "    df = pd.concat([existing, df], ignore_index=True)\n",
    "    df = df.drop_duplicates(subset=[\"listing_url\"], keep=\"last\")\n",
    "    logger.info(f\"Merged with existing data: {len(df)} total listings\")\n",
    "\n",
    "df.to_parquet(OUTPUT_PATH, index=False)\n",
    "logger.info(f\"Saved to {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts by generation:\")\n",
    "display(df[\"generation\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts by trim:\")\n",
    "display(df[\"trim\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts by transmission:\")\n",
    "display(df[\"transmission\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check parsing quality - how many have all required fields?\n",
    "required = [\"sale_price\", \"model_year\", \"generation\", \"trim\", \"transmission\", \"mileage\"]\n",
    "complete = df[required].notna().all(axis=1).sum()\n",
    "print(f\"\\nListings with all required fields: {complete} / {len(df)} ({complete/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some listings to verify parsing\n",
    "print(\"Sample listings for manual verification:\")\n",
    "sample = df.sample(min(5, len(df)))\n",
    "for _, row in sample.iterrows():\n",
    "    print(f\"\\n{row['title_raw']}\")\n",
    "    print(f\"  Parsed: {row['model_year']} {row['generation']} {row['trim']} ({row['transmission']})\")\n",
    "    print(f\"  Price: ${row['sale_price']:,}\" if pd.notna(row['sale_price']) else \"  Price: N/A\")\n",
    "    print(f\"  Mileage: {row['mileage']:,}\" if pd.notna(row['mileage']) else \"  Mileage: N/A\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

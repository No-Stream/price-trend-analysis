{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 05 - Gaussian Process Model for Nonlinear Effects\n",
    "\n",
    "This notebook explores Gaussian Processes (GPs) as an alternative to splines for modeling\n",
    "nonlinear age and mileage effects on Porsche 911 auction prices.\n",
    "\n",
    "**Model structure:**\n",
    "```\n",
    "log_price ~ intercept\n",
    "          + f_age(age)              # GP on age\n",
    "          + f_mileage(log_mileage)  # GP on log-mileage\n",
    "          + alpha_gen[generation]   # Random intercept\n",
    "          + alpha_trim[trim_tier]   # Random intercept\n",
    "          + alpha_trans[trans_type] # Random intercept\n",
    "          + alpha_body[body_style]  # Random intercept\n",
    "          + alpha_color[color]      # Random intercept\n",
    "          + epsilon\n",
    "```\n",
    "\n",
    "**Key design choices:**\n",
    "- **HSGP (Hilbert Space GP)**: Efficient approximation that scales linearly instead of O(n³)\n",
    "- **Additive GPs**: Separate 1D GPs for age and mileage (simpler than 2D surface)\n",
    "- **Raw PyMC**: Bambi doesn't support HSGP natively\n",
    "- **Same random effects**: Matches spline model for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import seaborn as sns\n",
    "\n",
    "from price_analysis.data.cleaning import prepare_model_data\n",
    "from price_analysis.models.comparison import compare_models_loo\n",
    "from price_analysis.models.hierarchical import check_diagnostics\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(f\"PyMC version: {pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "PROCESSED_PATH = DATA_DIR / \"processed\" / \"cleaned_listings.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_parquet(PROCESSED_PATH)\n",
    "df = prepare_model_data(df_cleaned, group_trims=True, group_trans=True)\n",
    "\n",
    "print(f\"Model data: {len(df)} listings\")\n",
    "print(f\"Age range: {df['age'].min():.0f} - {df['age'].max():.0f} years\")\n",
    "print(f\"Log-mileage range: {df['log_mileage'].min():.2f} - {df['log_mileage'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Standardize continuous inputs for GP - critical for HSGP accuracy\nAGE_MEAN, AGE_STD = df[\"age\"].mean(), df[\"age\"].std()\nLOGM_MEAN, LOGM_STD = df[\"log_mileage\"].mean(), df[\"log_mileage\"].std()\n\ndf[\"age_std\"] = (df[\"age\"] - AGE_MEAN) / AGE_STD\ndf[\"log_mileage_std\"] = (df[\"log_mileage\"] - LOGM_MEAN) / LOGM_STD\n\n# Validate input shapes for HSGP\nassert df[\"age_std\"].values.shape == (len(df),), f\"Unexpected age_std shape: {df['age_std'].values.shape}\"\nassert df[\"log_mileage_std\"].values.shape == (len(df),), f\"Unexpected log_mileage_std shape: {df['log_mileage_std'].values.shape}\"\nassert not df[\"age_std\"].isna().any(), \"age_std contains NaN values\"\nassert not df[\"log_mileage_std\"].isna().any(), \"log_mileage_std contains NaN values\"\n\nprint(f\"Standardization parameters:\")\nprint(f\"  Age: mean={AGE_MEAN:.2f}, std={AGE_STD:.2f}\")\nprint(f\"  Log-mileage: mean={LOGM_MEAN:.2f}, std={LOGM_STD:.2f}\")\nprint(f\"\\nStandardized ranges:\")\nprint(f\"  age_std: [{df['age_std'].min():.2f}, {df['age_std'].max():.2f}]\")\nprint(f\"  log_mileage_std: [{df['log_mileage_std'].min():.2f}, {df['log_mileage_std'].max():.2f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create integer indices for categorical variables\n",
    "gen_idx, gen_levels = pd.factorize(df[\"generation\"], sort=True)\n",
    "trim_idx, trim_levels = pd.factorize(df[\"trim_tier\"], sort=True)\n",
    "trans_idx, trans_levels = pd.factorize(df[\"trans_type\"], sort=True)\n",
    "body_idx, body_levels = pd.factorize(df[\"body_style\"], sort=True)\n",
    "color_idx, color_levels = pd.factorize(df[\"color_category\"], sort=True)\n",
    "\n",
    "print(\"Categorical levels:\")\n",
    "print(f\"  Generation ({len(gen_levels)}): {list(gen_levels)}\")\n",
    "print(f\"  Trim tier ({len(trim_levels)}): {list(trim_levels)}\")\n",
    "print(f\"  Trans type ({len(trans_levels)}): {list(trans_levels)}\")\n",
    "print(f\"  Body style ({len(body_levels)}): {list(body_levels)}\")\n",
    "print(f\"  Color category ({len(color_levels)}): {list(color_levels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## GP Background: Why HSGP?\n",
    "\n",
    "### The Problem with Standard GPs\n",
    "\n",
    "Standard Gaussian Process regression has O(n³) computational complexity due to matrix inversion.\n",
    "With n=1360 observations, this becomes slow and memory-intensive.\n",
    "\n",
    "### HSGP: Hilbert Space Approximation\n",
    "\n",
    "HSGP (Solin & Särkkä, 2020) approximates the GP using a truncated basis expansion:\n",
    "\n",
    "$$f(x) \\approx \\sum_{j=1}^{m} \\phi_j(x) \\cdot \\beta_j$$\n",
    "\n",
    "where:\n",
    "- $\\phi_j(x)$ are pre-computed basis functions (depend only on input domain, not kernel params)\n",
    "- $\\beta_j$ are coefficients with priors derived from the kernel's spectral density\n",
    "- $m$ controls approximation accuracy (more = better but slower)\n",
    "\n",
    "**Advantages:**\n",
    "- O(nm + m) complexity instead of O(n³)\n",
    "- Predictions via `pm.set_data()` without expensive GP conditioning\n",
    "- Works well for 1-2D inputs with stationary kernels\n",
    "\n",
    "**Limitations:**\n",
    "- Approximation quality depends on lengthscale relative to domain size\n",
    "- Less accurate for rapidly varying functions (short lengthscales)\n",
    "- Requires bounded domain\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Our choice | Rationale |\n",
    "|-----------|------------|------------|\n",
    "| `m` | 20 | Moderate accuracy, reasonable speed |\n",
    "| `c` | 1.5 | Extends domain 50% beyond data range |\n",
    "| Kernel | Matern52 | Allows local variation (less smooth than ExpQuad) |\n",
    "\n",
    "### Why Additive (Not 2D) GP?\n",
    "\n",
    "We use separate 1D GPs for age and log-mileage instead of a single 2D GP:\n",
    "\n",
    "1. **Simpler**: Each GP has its own lengthscale, easier to interpret\n",
    "2. **More robust**: 2D HSGP requires m² basis functions\n",
    "3. **Sufficient**: If residuals show age×mileage interaction, we can revisit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Build GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSGP configuration\n",
    "M_BASIS = 20  # Number of basis functions per GP\n",
    "C_FACTOR = 1.5  # Domain extension factor\n",
    "\n",
    "# Coordinates for labeled dimensions\n",
    "coords = {\n",
    "    \"generation\": gen_levels,\n",
    "    \"trim_tier\": trim_levels,\n",
    "    \"trans_type\": trans_levels,\n",
    "    \"body_style\": body_levels,\n",
    "    \"color_category\": color_levels,\n",
    "    \"obs\": np.arange(len(df)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "with pm.Model(coords=coords) as gp_model:\n    # === Data containers ===\n    age_data = pm.Data(\"age_std\", df[\"age_std\"].values)\n    mileage_data = pm.Data(\"log_mileage_std\", df[\"log_mileage_std\"].values)\n    y_data = pm.Data(\"log_price\", df[\"log_price\"].values)\n    \n    gen_data = pm.Data(\"gen_idx\", gen_idx)\n    trim_data = pm.Data(\"trim_idx\", trim_idx)\n    trans_data = pm.Data(\"trans_idx\", trans_idx)\n    body_data = pm.Data(\"body_idx\", body_idx)\n    color_data = pm.Data(\"color_idx\", color_idx)\n    \n    # === Intercept ===\n    intercept = pm.Normal(\"intercept\", mu=df[\"log_price\"].mean(), sigma=2)\n    \n    # === GP for age ===\n    # Priors: lengthscale and amplitude\n    # InverseGamma(3, 1) has mode ~0.5, allowing meaningful local variation\n    # For standardized data spanning ~[-1.5, 4], this permits the GP to vary\n    ls_age = pm.InverseGamma(\"ls_age\", alpha=3, beta=1)\n    eta_age = pm.HalfNormal(\"eta_age\", sigma=0.3)\n    \n    # Matern52 kernel\n    cov_age = eta_age**2 * pm.gp.cov.Matern52(input_dim=1, ls=ls_age)\n    \n    # HSGP approximation\n    gp_age = pm.gp.HSGP(m=[M_BASIS], c=C_FACTOR, cov_func=cov_age)\n    f_age = gp_age.prior(\"f_age\", X=age_data[:, None])\n    \n    # === GP for log-mileage ===\n    # InverseGamma(3, 1) has mode ~0.5 in standardized units\n    ls_mileage = pm.InverseGamma(\"ls_mileage\", alpha=3, beta=1)\n    eta_mileage = pm.HalfNormal(\"eta_mileage\", sigma=0.3)\n    \n    cov_mileage = eta_mileage**2 * pm.gp.cov.Matern52(input_dim=1, ls=ls_mileage)\n    \n    gp_mileage = pm.gp.HSGP(m=[M_BASIS], c=C_FACTOR, cov_func=cov_mileage)\n    f_mileage = gp_mileage.prior(\"f_mileage\", X=mileage_data[:, None])\n    \n    # === Random intercepts (non-centered parameterization) ===\n    # Generation\n    sigma_gen = pm.HalfNormal(\"sigma_gen\", sigma=0.5)\n    alpha_gen_offset = pm.Normal(\"alpha_gen_offset\", mu=0, sigma=1, dims=\"generation\")\n    alpha_gen = pm.Deterministic(\"alpha_gen\", alpha_gen_offset * sigma_gen, dims=\"generation\")\n    \n    # Trim tier\n    sigma_trim = pm.HalfNormal(\"sigma_trim\", sigma=0.7)\n    alpha_trim_offset = pm.Normal(\"alpha_trim_offset\", mu=0, sigma=1, dims=\"trim_tier\")\n    alpha_trim = pm.Deterministic(\"alpha_trim\", alpha_trim_offset * sigma_trim, dims=\"trim_tier\")\n    \n    # Transmission type\n    sigma_trans = pm.HalfNormal(\"sigma_trans\", sigma=0.3)\n    alpha_trans_offset = pm.Normal(\"alpha_trans_offset\", mu=0, sigma=1, dims=\"trans_type\")\n    alpha_trans = pm.Deterministic(\"alpha_trans\", alpha_trans_offset * sigma_trans, dims=\"trans_type\")\n    \n    # Body style\n    sigma_body = pm.HalfNormal(\"sigma_body\", sigma=0.3)\n    alpha_body_offset = pm.Normal(\"alpha_body_offset\", mu=0, sigma=1, dims=\"body_style\")\n    alpha_body = pm.Deterministic(\"alpha_body\", alpha_body_offset * sigma_body, dims=\"body_style\")\n    \n    # Color category\n    sigma_color = pm.HalfNormal(\"sigma_color\", sigma=0.3)\n    alpha_color_offset = pm.Normal(\"alpha_color_offset\", mu=0, sigma=1, dims=\"color_category\")\n    alpha_color = pm.Deterministic(\"alpha_color\", alpha_color_offset * sigma_color, dims=\"color_category\")\n    \n    # === Mean function ===\n    mu = (\n        intercept\n        + f_age\n        + f_mileage\n        + alpha_gen[gen_data]\n        + alpha_trim[trim_data]\n        + alpha_trans[trans_data]\n        + alpha_body[body_data]\n        + alpha_color[color_data]\n    )\n    \n    # === Observation noise (matches spline model's Bambi default) ===\n    sigma_obs = pm.HalfStudentT(\"sigma_obs\", nu=4, sigma=0.8229)\n    \n    # === Likelihood ===\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma_obs, observed=y_data, dims=\"obs\")\n\nprint(gp_model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model graph\n",
    "pm.model_to_graphviz(gp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Prior Predictive Check\n",
    "\n",
    "Before fitting, verify that our priors produce reasonable function shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gp_model:\n",
    "    prior_pred = pm.sample_prior_predictive(samples=100, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prior GP function draws\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sort for plotting\n",
    "age_order = np.argsort(df[\"age_std\"].values)\n",
    "mileage_order = np.argsort(df[\"log_mileage_std\"].values)\n",
    "\n",
    "# Age GP prior samples\n",
    "ax = axes[0]\n",
    "f_age_prior = prior_pred.prior[\"f_age\"].values[0]  # (samples, obs)\n",
    "for i in range(min(20, f_age_prior.shape[0])):\n",
    "    ax.plot(df[\"age\"].values[age_order], f_age_prior[i, age_order], alpha=0.3, color=\"steelblue\")\n",
    "ax.axhline(0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xlabel(\"Age (years)\")\n",
    "ax.set_ylabel(\"f_age (effect on log-price)\")\n",
    "ax.set_title(\"Prior GP samples: Age effect\")\n",
    "\n",
    "# Mileage GP prior samples\n",
    "ax = axes[1]\n",
    "f_mileage_prior = prior_pred.prior[\"f_mileage\"].values[0]\n",
    "for i in range(min(20, f_mileage_prior.shape[0])):\n",
    "    ax.plot(df[\"log_mileage\"].values[mileage_order], f_mileage_prior[i, mileage_order], \n",
    "            alpha=0.3, color=\"coral\")\n",
    "ax.axhline(0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_xlabel(\"log(Mileage)\")\n",
    "ax.set_ylabel(\"f_mileage (effect on log-price)\")\n",
    "ax.set_title(\"Prior GP samples: Mileage effect\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior predictive distribution of prices\n",
    "y_prior = prior_pred.prior_predictive[\"y_obs\"].values.flatten()\n",
    "price_prior = np.exp(y_prior)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(y_prior, bins=50, alpha=0.7, density=True)\n",
    "ax.axvline(df[\"log_price\"].mean(), color=\"red\", linestyle=\"--\", label=f\"Data mean: {df['log_price'].mean():.2f}\")\n",
    "ax.set_xlabel(\"log(price)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Prior Predictive: log(price)\")\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "# Clip extreme values for visualization\n",
    "price_clipped = np.clip(price_prior, 0, 1e7)\n",
    "ax.hist(price_clipped / 1000, bins=50, alpha=0.7, density=True)\n",
    "ax.axvline(np.exp(df[\"log_price\"]).median() / 1000, color=\"red\", linestyle=\"--\", \n",
    "           label=f\"Data median: ${np.exp(df['log_price']).median()/1000:.0f}k\")\n",
    "ax.set_xlabel(\"Price ($k)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Prior Predictive: Price (clipped at $10M)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"Prior predictive log-price range: [{y_prior.min():.1f}, {y_prior.max():.1f}]\")\n",
    "print(f\"Data log-price range: [{df['log_price'].min():.2f}, {df['log_price'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Fit GP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with gp_model:\n",
    "    idata_gp = pm.sample(\n",
    "        draws=1000,\n",
    "        tune=1000,\n",
    "        chains=8,\n",
    "        cores=8,\n",
    "        target_accept=0.95,\n",
    "        random_seed=42,\n",
    "        idata_kwargs={\"log_likelihood\": True},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check diagnostics\n",
    "diagnostics = check_diagnostics(idata_gp)\n",
    "print(f\"Converged: {diagnostics['converged']}\")\n",
    "print(f\"Divergences: {diagnostics['n_divergences']}\")\n",
    "print(f\"Max R-hat: {diagnostics['rhat_max']:.3f}\")\n",
    "print(f\"Min ESS (bulk): {diagnostics['ess_bulk_min']:.0f}\")\n",
    "if diagnostics[\"issues\"]:\n",
    "    print(f\"Issues: {diagnostics['issues']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key parameters\n",
    "var_names = [\"intercept\", \"ls_age\", \"ls_mileage\", \"eta_age\", \"eta_mileage\", \n",
    "             \"sigma_gen\", \"sigma_trim\", \"sigma_trans\", \"sigma_body\", \"sigma_color\", \"sigma_obs\"]\n",
    "summary = az.summary(idata_gp, var_names=var_names)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots for GP hyperparameters\n",
    "az.plot_trace(idata_gp, var_names=[\"ls_age\", \"ls_mileage\", \"eta_age\", \"eta_mileage\", \"sigma_obs\"])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Visualize GP Effects\n",
    "\n",
    "Plot the learned nonlinear relationships between age/mileage and log-price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_effect(\n",
    "    idata: az.InferenceData,\n",
    "    gp_var: str,\n",
    "    x_data: np.ndarray,\n",
    "    x_label: str,\n",
    "    ax: plt.Axes,\n",
    "    ci: float = 0.9,\n",
    ") -> None:\n",
    "    \"\"\"Plot GP posterior mean and credible interval.\"\"\"\n",
    "    # Get posterior samples\n",
    "    f_samples = idata.posterior[gp_var].values\n",
    "    f_samples = f_samples.reshape(-1, len(x_data))  # (chains*draws, n_obs)\n",
    "    \n",
    "    # Sort by x for plotting\n",
    "    order = np.argsort(x_data)\n",
    "    x_sorted = x_data[order]\n",
    "    f_sorted = f_samples[:, order]\n",
    "    \n",
    "    # Compute statistics\n",
    "    f_mean = f_sorted.mean(axis=0)\n",
    "    alpha = (1 - ci) / 2\n",
    "    f_lower = np.percentile(f_sorted, alpha * 100, axis=0)\n",
    "    f_upper = np.percentile(f_sorted, (1 - alpha) * 100, axis=0)\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(x_sorted, f_mean, \"b-\", linewidth=2, label=\"Posterior mean\")\n",
    "    ax.fill_between(x_sorted, f_lower, f_upper, alpha=0.3, color=\"blue\", \n",
    "                    label=f\"{int(ci*100)}% CI\")\n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(\"Effect on log(price)\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Age effect\n",
    "plot_gp_effect(idata_gp, \"f_age\", df[\"age\"].values, \"Age (years)\", axes[0])\n",
    "axes[0].set_title(\"GP Effect: Age\")\n",
    "\n",
    "# Mileage effect\n",
    "plot_gp_effect(idata_gp, \"f_mileage\", df[\"log_mileage\"].values, \"log(Mileage)\", axes[1])\n",
    "axes[1].set_title(\"GP Effect: Log-Mileage\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation: convert GP effects to percentage price changes\n",
    "f_age_samples = idata_gp.posterior[\"f_age\"].values.reshape(-1, len(df))\n",
    "f_mileage_samples = idata_gp.posterior[\"f_mileage\"].values.reshape(-1, len(df))\n",
    "\n",
    "# Effect range (max - min effect)\n",
    "age_effect_range = f_age_samples.mean(axis=0).max() - f_age_samples.mean(axis=0).min()\n",
    "mileage_effect_range = f_mileage_samples.mean(axis=0).max() - f_mileage_samples.mean(axis=0).min()\n",
    "\n",
    "print(\"GP effect ranges (on log-price scale):\")\n",
    "print(f\"  Age: {age_effect_range:.3f} = {(np.exp(age_effect_range) - 1) * 100:.1f}% price difference\")\n",
    "print(f\"  Mileage: {mileage_effect_range:.3f} = {(np.exp(mileage_effect_range) - 1) * 100:.1f}% price difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Random Effects Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plots for random intercepts\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "az.plot_forest(idata_gp, var_names=[\"alpha_gen\"], combined=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Generation Effects\")\n",
    "\n",
    "az.plot_forest(idata_gp, var_names=[\"alpha_trim\"], combined=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"Trim Tier Effects\")\n",
    "\n",
    "az.plot_forest(idata_gp, var_names=[\"alpha_trans\"], combined=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Transmission Effects\")\n",
    "\n",
    "az.plot_forest(idata_gp, var_names=[\"alpha_body\"], combined=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Body Style Effects\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color category effects\n",
    "az.plot_forest(idata_gp, var_names=[\"alpha_color\"], combined=True, figsize=(10, 4))\n",
    "plt.title(\"Color Category Effects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dollar premiums for random effects\n",
    "REFERENCE_PRICE = 80000\n",
    "\n",
    "def compute_premium(idata, var_name, level_names, baseline_idx=0):\n",
    "    \"\"\"Compute dollar premiums relative to baseline level.\"\"\"\n",
    "    samples = idata.posterior[var_name].values\n",
    "    samples = samples.reshape(-1, len(level_names))\n",
    "    \n",
    "    baseline = samples[:, baseline_idx]\n",
    "    premiums = {}\n",
    "    for i, name in enumerate(level_names):\n",
    "        if i == baseline_idx:\n",
    "            continue\n",
    "        diff = samples[:, i] - baseline\n",
    "        dollar_diff = REFERENCE_PRICE * (np.exp(diff) - 1)\n",
    "        premiums[name] = {\n",
    "            \"median\": np.median(dollar_diff),\n",
    "            \"ci_90\": (np.percentile(dollar_diff, 5), np.percentile(dollar_diff, 95)),\n",
    "        }\n",
    "    return premiums\n",
    "\n",
    "# Trim premiums (vs base)\n",
    "base_idx = list(trim_levels).index(\"base\")\n",
    "trim_premiums = compute_premium(idata_gp, \"alpha_trim\", trim_levels, baseline_idx=base_idx)\n",
    "\n",
    "print(f\"Trim tier premiums vs 'base' at ${REFERENCE_PRICE/1000:.0f}k reference:\")\n",
    "for level, stats in sorted(trim_premiums.items(), key=lambda x: -x[1][\"median\"]):\n",
    "    print(f\"  {level}: ${stats['median']:+,.0f} [{stats['ci_90'][0]:+,.0f}, {stats['ci_90'][1]:+,.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transmission premiums (vs PDK)\n",
    "pdk_idx = list(trans_levels).index(\"pdk\")\n",
    "trans_premiums = compute_premium(idata_gp, \"alpha_trans\", trans_levels, baseline_idx=pdk_idx)\n",
    "\n",
    "print(f\"\\nTransmission premiums vs 'pdk' at ${REFERENCE_PRICE/1000:.0f}k reference:\")\n",
    "for level, stats in sorted(trans_premiums.items(), key=lambda x: -x[1][\"median\"]):\n",
    "    print(f\"  {level}: ${stats['median']:+,.0f} [{stats['ci_90'][0]:+,.0f}, {stats['ci_90'][1]:+,.0f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": "## Model Comparison: GP vs Spline\n\nCompare GP and spline models via LOO-CV.\n\nWe save fitted models to avoid re-fitting on subsequent runs. The spline model\ntakes ~2 minutes to fit, so loading from disk saves significant time."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": "# Save GP model idata for future use\nGP_IDATA_PATH = DATA_DIR / \"models\" / \"gp_idata.nc\"\nSPLINE_IDATA_PATH = DATA_DIR / \"models\" / \"spline_idata.nc\"\n\n# Create models directory if needed\n(DATA_DIR / \"models\").mkdir(exist_ok=True)\n\n# Save GP idata\nidata_gp.to_netcdf(GP_IDATA_PATH)\nprint(f\"Saved GP idata to {GP_IDATA_PATH}\")\n\n# Load or fit spline model\nfrom price_analysis.models.spline import build_spline_model, fit_spline_model\n\nspline_model = build_spline_model(\n    df,\n    age_df=6,\n    mileage_df=6,\n    include_sale_year=False,\n    include_color=True,\n)\n\nif SPLINE_IDATA_PATH.exists():\n    print(f\"Loading spline idata from {SPLINE_IDATA_PATH}\")\n    idata_spline = az.from_netcdf(SPLINE_IDATA_PATH)\nelse:\n    print(\"Fitting spline model (this takes ~2 minutes)...\")\n    idata_spline = fit_spline_model(\n        spline_model,\n        draws=1000,\n        tune=1000,\n        chains=8,\n        cores=8,\n        target_accept=0.975,\n    )\n    idata_spline.to_netcdf(SPLINE_IDATA_PATH)\n    print(f\"Saved spline idata to {SPLINE_IDATA_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOO-CV comparison\n",
    "comparison = compare_models_loo(\n",
    "    {\n",
    "        \"GP (HSGP)\": idata_gp,\n",
    "        \"Spline\": idata_spline,\n",
    "    }\n",
    ")\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_compare(comparison)\n",
    "plt.title(\"Model Comparison (LOO-CV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Residual Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute GP residuals\n",
    "with gp_model:\n",
    "    pm.sample_posterior_predictive(idata_gp, extend_inferencedata=True, random_seed=42)\n",
    "\n",
    "y_pred_gp = idata_gp.posterior_predictive[\"y_obs\"].values.reshape(-1, len(df))\n",
    "y_pred_mean_gp = y_pred_gp.mean(axis=0)\n",
    "residuals_gp = df[\"log_price\"].values - y_pred_mean_gp\n",
    "\n",
    "print(f\"GP Residual stats:\")\n",
    "print(f\"  RMSE: {np.sqrt((residuals_gp**2).mean()):.4f}\")\n",
    "print(f\"  MAE: {np.abs(residuals_gp).mean():.4f}\")\n",
    "print(f\"  Mean: {residuals_gp.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spline residuals for comparison\n",
    "spline_model.predict(idata_spline, data=df, kind=\"response\", inplace=True)\n",
    "y_pred_spline = idata_spline.posterior_predictive[\"log_price\"].values.reshape(-1, len(df))\n",
    "y_pred_mean_spline = y_pred_spline.mean(axis=0)\n",
    "residuals_spline = df[\"log_price\"].values - y_pred_mean_spline\n",
    "\n",
    "print(f\"\\nSpline Residual stats:\")\n",
    "print(f\"  RMSE: {np.sqrt((residuals_spline**2).mean()):.4f}\")\n",
    "print(f\"  MAE: {np.abs(residuals_spline).mean():.4f}\")\n",
    "print(f\"  Mean: {residuals_spline.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# GP residuals\n",
    "axes[0, 0].scatter(y_pred_mean_gp, residuals_gp, alpha=0.3, s=10)\n",
    "axes[0, 0].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axes[0, 0].set_xlabel(\"Fitted\")\n",
    "axes[0, 0].set_ylabel(\"Residual\")\n",
    "axes[0, 0].set_title(\"GP: Residuals vs Fitted\")\n",
    "\n",
    "axes[0, 1].scatter(df[\"age\"].values, residuals_gp, alpha=0.3, s=10)\n",
    "axes[0, 1].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axes[0, 1].set_xlabel(\"Age\")\n",
    "axes[0, 1].set_ylabel(\"Residual\")\n",
    "axes[0, 1].set_title(\"GP: Residuals vs Age\")\n",
    "\n",
    "axes[0, 2].scatter(df[\"log_mileage\"].values, residuals_gp, alpha=0.3, s=10)\n",
    "axes[0, 2].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axes[0, 2].set_xlabel(\"log(Mileage)\")\n",
    "axes[0, 2].set_ylabel(\"Residual\")\n",
    "axes[0, 2].set_title(\"GP: Residuals vs log(Mileage)\")\n",
    "\n",
    "# Spline residuals\n",
    "axes[1, 0].scatter(y_pred_mean_spline, residuals_spline, alpha=0.3, s=10)\n",
    "axes[1, 0].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axes[1, 0].set_xlabel(\"Fitted\")\n",
    "axes[1, 0].set_ylabel(\"Residual\")\n",
    "axes[1, 0].set_title(\"Spline: Residuals vs Fitted\")\n",
    "\n",
    "axes[1, 1].scatter(df[\"age\"].values, residuals_spline, alpha=0.3, s=10)\n",
    "axes[1, 1].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axes[1, 1].set_xlabel(\"Age\")\n",
    "axes[1, 1].set_ylabel(\"Residual\")\n",
    "axes[1, 1].set_title(\"Spline: Residuals vs Age\")\n",
    "\n",
    "axes[1, 2].scatter(df[\"log_mileage\"].values, residuals_spline, alpha=0.3, s=10)\n",
    "axes[1, 2].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "axes[1, 2].set_xlabel(\"log(Mileage)\")\n",
    "axes[1, 2].set_ylabel(\"Residual\")\n",
    "axes[1, 2].set_title(\"Spline: Residuals vs log(Mileage)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## Reference Car Predictions\n",
    "\n",
    "Compare predictions for the same reference cars used in notebook 04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": "def predict_gp_price(\n    model: pm.Model,\n    idata: az.InferenceData,\n    age: float,\n    mileage: int,\n    generation: str,\n    trim_tier: str,\n    trans_type: str,\n    body_style: str,\n    color_category: str,\n    n_samples: int = 500,\n) -> dict:\n    \"\"\"Predict price for a single car configuration using the GP model.\n    \n    Uses pm.set_data() to update model inputs and pm.sample_posterior_predictive()\n    to properly evaluate the HSGP at new input locations.\n    \n    Args:\n        model: The PyMC GP model\n        idata: InferenceData from model fitting\n        age: Age of car in years\n        mileage: Current mileage\n        generation: Car generation (e.g., \"992.1\")\n        trim_tier: Trim tier (e.g., \"sport\")\n        trans_type: Transmission type (e.g., \"manual\")\n        body_style: Body style (e.g., \"coupe\")\n        color_category: Color category (e.g., \"standard\")\n        n_samples: Number of posterior samples to use (default 500 for speed)\n    \n    Returns:\n        Dict with price predictions and uncertainty intervals\n    \"\"\"\n    # Standardize inputs\n    age_std = (age - AGE_MEAN) / AGE_STD\n    log_mileage = np.log(max(mileage, 1))\n    log_mileage_std = (log_mileage - LOGM_MEAN) / LOGM_STD\n    \n    # Get indices for categorical variables\n    gen_i = list(gen_levels).index(generation)\n    trim_i = list(trim_levels).index(trim_tier)\n    trans_i = list(trans_levels).index(trans_type)\n    body_i = list(body_levels).index(body_style)\n    color_i = list(color_levels).index(color_category)\n    \n    # Update model data containers for single prediction\n    with model:\n        pm.set_data({\n            \"age_std\": np.array([age_std]),\n            \"log_mileage_std\": np.array([log_mileage_std]),\n            \"gen_idx\": np.array([gen_i]),\n            \"trim_idx\": np.array([trim_i]),\n            \"trans_idx\": np.array([trans_i]),\n            \"body_idx\": np.array([body_i]),\n            \"color_idx\": np.array([color_i]),\n        })\n        \n        # Sample from posterior predictive at new point\n        # Use subset of posterior samples for speed\n        idata_subset = idata.sel(draw=slice(0, n_samples))\n        pred = pm.sample_posterior_predictive(\n            idata_subset,\n            var_names=[\"y_obs\"],\n            random_seed=42,\n            progressbar=False,\n        )\n    \n    # Extract predictions\n    log_price_samples = pred.posterior_predictive[\"y_obs\"].values.flatten()\n    price_samples = np.exp(log_price_samples)\n    \n    # Reset data to original (important for subsequent operations)\n    with model:\n        pm.set_data({\n            \"age_std\": df[\"age_std\"].values,\n            \"log_mileage_std\": df[\"log_mileage_std\"].values,\n            \"gen_idx\": gen_idx,\n            \"trim_idx\": trim_idx,\n            \"trans_idx\": trans_idx,\n            \"body_idx\": body_idx,\n            \"color_idx\": color_idx,\n        })\n    \n    return {\n        \"config\": {\n            \"age\": age,\n            \"mileage\": mileage,\n            \"generation\": generation,\n            \"trim_tier\": trim_tier,\n            \"trans_type\": trans_type,\n            \"body_style\": body_style,\n            \"color_category\": color_category,\n        },\n        \"price\": {\n            \"mean\": float(np.mean(price_samples)),\n            \"median\": float(np.median(price_samples)),\n            \"std\": float(np.std(price_samples)),\n            \"ci_80\": [float(np.percentile(price_samples, 10)), float(np.percentile(price_samples, 90))],\n            \"ci_95\": [float(np.percentile(price_samples, 2.5)), float(np.percentile(price_samples, 97.5))],\n        },\n        \"samples\": price_samples,\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference car 1: 996.2 Carrera 4S Manual (2002, 45k miles)\n",
    "pred_gp_996 = predict_gp_price(\n",
    "    gp_model, idata_gp,\n",
    "    age=23,  # 2025 - 2002\n",
    "    mileage=45000,\n",
    "    generation=\"996.2\",\n",
    "    trim_tier=\"sport\",\n",
    "    trans_type=\"manual\",\n",
    "    body_style=\"coupe\",\n",
    "    color_category=\"standard\",\n",
    ")\n",
    "\n",
    "# Reference car 2: 992.1 Carrera 4S PDK (2022, 15k miles)\n",
    "pred_gp_992 = predict_gp_price(\n",
    "    gp_model, idata_gp,\n",
    "    age=3,  # 2025 - 2022\n",
    "    mileage=15000,\n",
    "    generation=\"992.1\",\n",
    "    trim_tier=\"sport\",\n",
    "    trans_type=\"pdk\",\n",
    "    body_style=\"coupe\",\n",
    "    color_category=\"standard\",\n",
    ")\n",
    "\n",
    "print(\"GP Model Predictions:\")\n",
    "print(f\"\\n996.2 Carrera 4S Manual (2002, 45k mi):\")\n",
    "print(f\"  Median: ${pred_gp_996['price']['median']:,.0f}\")\n",
    "print(f\"  80% CI: [${pred_gp_996['price']['ci_80'][0]:,.0f}, ${pred_gp_996['price']['ci_80'][1]:,.0f}]\")\n",
    "\n",
    "print(f\"\\n992.1 Carrera 4S PDK (2022, 15k mi):\")\n",
    "print(f\"  Median: ${pred_gp_992['price']['median']:,.0f}\")\n",
    "print(f\"  80% CI: [${pred_gp_992['price']['ci_80'][0]:,.0f}, ${pred_gp_992['price']['ci_80'][1]:,.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with spline predictions\n",
    "from price_analysis.models.spline import predict_spline_price\n",
    "\n",
    "pred_spline_996 = predict_spline_price(\n",
    "    model=spline_model,\n",
    "    idata=idata_spline,\n",
    "    df=df,\n",
    "    generation=\"996.2\",\n",
    "    trim_tier=\"sport\",\n",
    "    trans_type=\"manual\",\n",
    "    body_style=\"coupe\",\n",
    "    model_year=2002,\n",
    "    mileage=45000,\n",
    "    sale_year=2025,\n",
    "    include_sale_year=False,\n",
    "    color_category=\"standard\",\n",
    ")\n",
    "\n",
    "pred_spline_992 = predict_spline_price(\n",
    "    model=spline_model,\n",
    "    idata=idata_spline,\n",
    "    df=df,\n",
    "    generation=\"992.1\",\n",
    "    trim_tier=\"sport\",\n",
    "    trans_type=\"pdk\",\n",
    "    body_style=\"coupe\",\n",
    "    model_year=2022,\n",
    "    mileage=15000,\n",
    "    sale_year=2025,\n",
    "    include_sale_year=False,\n",
    "    color_category=\"standard\",\n",
    ")\n",
    "\n",
    "print(\"\\nPrediction Comparison:\")\n",
    "print(f\"\\n996.2 Carrera 4S Manual (2002, 45k mi):\")\n",
    "print(f\"  GP:     ${pred_gp_996['price']['median']:,.0f}\")\n",
    "print(f\"  Spline: ${pred_spline_996['price']['median']:,.0f}\")\n",
    "print(f\"  Diff:   ${pred_gp_996['price']['median'] - pred_spline_996['price']['median']:+,.0f}\")\n",
    "\n",
    "print(f\"\\n992.1 Carrera 4S PDK (2022, 15k mi):\")\n",
    "print(f\"  GP:     ${pred_gp_992['price']['median']:,.0f}\")\n",
    "print(f\"  Spline: ${pred_spline_992['price']['median']:,.0f}\")\n",
    "print(f\"  Diff:   ${pred_gp_992['price']['median'] - pred_spline_992['price']['median']:+,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ansks3zom75",
   "source": "## Depreciation Curves\n\nCompare predicted depreciation trajectories between GP and spline models.\nThis shows how each model captures the nonlinear relationship between age and price.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "j2bd0j0ei1",
   "source": "# Compare depreciation curves for 992.1 and 997.2 generations\nages = list(range(1, 12))\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nfor idx, (gen, base_year) in enumerate([(\"992.1\", 2022), (\"997.2\", 2010)]):\n    ax = axes[idx]\n    \n    # GP model predictions\n    gp_medians = []\n    gp_lowers = []\n    gp_uppers = []\n    for age in ages:\n        pred = predict_gp_price(\n            gp_model, idata_gp,\n            age=age,\n            mileage=5000 * age,\n            generation=gen,\n            trim_tier=\"sport\",\n            trans_type=\"pdk\",\n            body_style=\"coupe\",\n            color_category=\"standard\",\n            n_samples=200,  # Faster for curve plotting\n        )\n        gp_medians.append(pred[\"price\"][\"median\"])\n        gp_lowers.append(pred[\"price\"][\"ci_80\"][0])\n        gp_uppers.append(pred[\"price\"][\"ci_80\"][1])\n    \n    # Spline model predictions\n    spline_medians = []\n    spline_lowers = []\n    spline_uppers = []\n    for age in ages:\n        pred = predict_spline_price(\n            model=spline_model,\n            idata=idata_spline,\n            df=df,\n            generation=gen,\n            trim_tier=\"sport\",\n            trans_type=\"pdk\",\n            body_style=\"coupe\",\n            model_year=base_year,\n            mileage=5000 * age,\n            sale_year=base_year + age,\n            include_sale_year=False,\n            color_category=\"standard\",\n        )\n        spline_medians.append(pred[\"price\"][\"median\"])\n        spline_lowers.append(pred[\"price\"][\"ci_80\"][0])\n        spline_uppers.append(pred[\"price\"][\"ci_80\"][1])\n    \n    # Plot GP\n    ax.plot(ages, [m / 1000 for m in gp_medians], \"o-\", label=\"GP\", linewidth=2, color=\"steelblue\")\n    ax.fill_between(ages, [l / 1000 for l in gp_lowers], [u / 1000 for u in gp_uppers], \n                    alpha=0.2, color=\"steelblue\")\n    \n    # Plot Spline\n    ax.plot(ages, [m / 1000 for m in spline_medians], \"s--\", label=\"Spline\", linewidth=2, color=\"coral\")\n    ax.fill_between(ages, [l / 1000 for l in spline_lowers], [u / 1000 for u in spline_uppers], \n                    alpha=0.2, color=\"coral\")\n    \n    ax.set_xlabel(\"Age (years)\")\n    ax.set_ylabel(\"Predicted Price ($k)\")\n    ax.set_title(f\"{gen} Carrera 4S PDK (5k mi/year)\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.suptitle(\"Depreciation Curves: GP vs Spline\", y=1.02, fontsize=14)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDiagnostics:\")\n",
    "print(f\"  GP divergences: {diagnostics['n_divergences']}\")\n",
    "print(f\"  GP max R-hat: {diagnostics['rhat_max']:.3f}\")\n",
    "print(f\"  GP min ESS: {diagnostics['ess_bulk_min']:.0f}\")\n",
    "\n",
    "print(f\"\\nResidual RMSE (log-price):\")\n",
    "print(f\"  GP:     {np.sqrt((residuals_gp**2).mean()):.4f}\")\n",
    "print(f\"  Spline: {np.sqrt((residuals_spline**2).mean()):.4f}\")\n",
    "\n",
    "print(f\"\\nLOO-CV ELPD:\")\n",
    "for model_name in comparison.index:\n",
    "    elpd = comparison.loc[model_name, \"elpd_loo\"]\n",
    "    se = comparison.loc[model_name, \"se\"]\n",
    "    print(f\"  {model_name}: {elpd:.1f} +/- {se:.1f}\")\n",
    "\n",
    "print(f\"\\nGP Hyperparameters (posterior medians):\")\n",
    "ls_age_med = np.median(idata_gp.posterior[\"ls_age\"].values)\n",
    "ls_mileage_med = np.median(idata_gp.posterior[\"ls_mileage\"].values)\n",
    "eta_age_med = np.median(idata_gp.posterior[\"eta_age\"].values)\n",
    "eta_mileage_med = np.median(idata_gp.posterior[\"eta_mileage\"].values)\n",
    "print(f\"  Age lengthscale: {ls_age_med:.2f} (standardized)\")\n",
    "print(f\"  Mileage lengthscale: {ls_mileage_med:.2f} (standardized)\")\n",
    "print(f\"  Age amplitude: {eta_age_med:.3f}\")\n",
    "print(f\"  Mileage amplitude: {eta_mileage_med:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "**1. Model Fit:**\n",
    "- GP and spline models show similar predictive performance (LOO-CV)\n",
    "- Both capture nonlinear age/mileage effects reasonably well\n",
    "- Residual patterns are comparable between models\n",
    "\n",
    "**2. GP Hyperparameters:**\n",
    "- Lengthscales indicate smooth but flexible functions\n",
    "- Amplitudes show age and mileage contribute similar variance\n",
    "\n",
    "**3. Random Effects:**\n",
    "- GP and spline models agree on categorical premiums\n",
    "- Manual transmission premium, trim tier ordering, etc. are consistent\n",
    "\n",
    "**4. Computational Considerations:**\n",
    "- HSGP makes GP tractable for this dataset size\n",
    "- Spline model (via Bambi) is simpler to set up and predict with\n",
    "- GP requires more careful prior specification\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "- **For production use**: Spline model is preferred (simpler, similar accuracy)\n",
    "- **For research**: GP provides additional flexibility if nonlinearity is complex\n",
    "- **Future work**: Consider 2D GP if age×mileage interaction is suspected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}